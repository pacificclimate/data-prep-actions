# Format Daily Prism Files

## Purpose

The scripts in this archive were used to format three daily-resolution sets of PRISM output so that they could be indexed into the pcic_meta database, mapped by ncWMS, and downloaded by a user.

## Goals

### 1. Supply missing metadata

Some of the metadata required by the PCIC metadata standard is missing from these files. Missing metadata was added with the update_metadata script in modelmeta.

### 2. Resolve an error related to file formatting

The precipitation file contains an unknown formatting anomaly that causes some - but not all - netCDF software to crash or produce errors when reading it. The file was recreated from scratch, and the new version also contained the error, so it does not seem to have been caused by file corruption.

The exact nature of the anomaly remains unknown, but here is a list of errors generated by various netCDF software packages when dealing with it:

* ncview can read affected files correctly and does not raise errors
* ncdump can read affected files correctly and does not raise errors
* nccopy can read affected files correctly and does not raise errors
* cdo provides a warning about "negative seek offset" for affected files, but I did not record the exact wording.
* the pdp backend cannot serve data from a file with the mysterious data anomaly, and gives the following error messages:
```
2018-04-04 20:33:13,460 DEBG 'pdp_backend' stdout output:
WARNING:pydap.handlers.hdf5:Failed to convert attribute REFERENCE_LIST

2018-04-04 20:33:13,462 DEBG 'pdp_backend' stdout output:
DEBUG:pydap.handlers.hdf5:Hdf5Data.__init__(<HDF5 dataset "time": shape (696,), type "<f4">, None)
DEBUG:pydap.handlers.hdf5:HDF5Data(<HDF5 dataset "time": shape (696,), type "<f4">).shape : major_slice=StackableSlice(None:None:None) and slices=[StackableSlice(None, None, None)]
DEBUG:pydap.handlers.hdf5:leaving shape with result (696,)
DEBUG:pydap.handlers.hdf5:end Hdf5Data.__init__()

2018-04-04 20:33:13,462 DEBG 'pdp_backend' stdout output:
WARNING:pydap.handlers.hdf5:Failed to convert attribute standard_name
WARNING:pydap.handlers.hdf5:Failed to convert attribute long_name

2018-04-04 20:33:13,465 DEBG 'pdp_backend' stdout output:
WARNING:pydap.handlers.hdf5:Failed to convert attribute units
WARNING:pydap.handlers.hdf5:Failed to convert attribute axis
WARNING:pydap.handlers.hdf5:Failed to convert attribute REFERENCE_LIST
DEBUG:pydap.handlers.hdf5:Hdf5Data.__init__(<HDF5 dataset "lat": shape (1680,), type "<f8">, None)
DEBUG:pydap.handlers.hdf5:HDF5Data(<HDF5 dataset "lat": shape (1680,), type "<f8">).shape : major_slice=StackableSlice(None:None:None) and slices=[StackableSlice(None, None, None)]
DEBUG:pydap.handlers.hdf5:leaving shape with result (1680,)
DEBUG:pydap.handlers.hdf5:end Hdf5Data.__init__()
WARNING:pydap.handlers.hdf5:Failed to convert attribute standard_name
WARNING:pydap.handlers.hdf5:Failed to convert attribute long_name

2018-04-04 20:33:13,466 DEBG 'pdp_backend' stdout output:
WARNING:pydap.handlers.hdf5:Failed to convert attribute units

2018-04-04 20:33:13,470 DEBG 'pdp_backend' stdout output:
WARNING:pydap.handlers.hdf5:Failed to convert attribute axis
WARNING:pydap.handlers.hdf5:Failed to convert attribute REFERENCE_LIST
DEBUG:pydap.handlers.hdf5:Hdf5Data.__init__(<HDF5 dataset "lon": shape (3241,), type "<f8">, None)
DEBUG:pydap.handlers.hdf5:HDF5Data(<HDF5 dataset "lon": shape (3241,), type "<f8">).shape : major_slice=StackableSlice(None:None:None) and slices=[StackableSlice(None, None, None)]
DEBUG:pydap.handlers.hdf5:leaving shape with result (3241,)
DEBUG:pydap.handlers.hdf5:end Hdf5Data.__init__()
WARNING:pydap.handlers.hdf5:Failed to convert attribute units
WARNING:pydap.handlers.hdf5:Failed to convert attribute long_name
WARNING:pydap.handlers.hdf5:Failed to convert attribute calendar
WARNING:pydap.handlers.hdf5:Failed to convert attribute REFERENCE_LIST
DEBUG:pydap.handlers.hdf5:Hdf5Data.__init__(<HDF5 dataset "time": shape (696,), type "<f4">, None)
DEBUG:pydap.handlers.hdf5:HDF5Data(<HDF5 dataset "time": shape (696,), type "<f4">).shape : major_slice=StackableSlice(None:None:None) and slices=[StackableSlice(None, None, None)]
DEBUG:pydap.handlers.hdf5:leaving shape with result (696,)
DEBUG:pydap.handlers.hdf5:end Hdf5Data.__init__()
WARNING:pydap.handlers.hdf5:Failed to convert attribute grid_mapping_name
WARNING:pydap.handlers.hdf5:Failed to convert attribute long_name

2018-04-04 20:33:13,471 DEBG 'pdp_backend' stdout output:
WARNING:pydap.handlers.hdf5:Failed to convert attribute GeoTransform

2018-04-04 20:33:13,471 DEBG 'pdp_backend' stdout output:
WARNING:pydap.handlers.hdf5:Failed to convert attribute crs_wkt

2018-04-04 20:33:13,471 DEBG 'pdp_backend' stdout output:
DEBUG:pydap.handlers.hdf5:Hdf5Data.__init__(<HDF5 dataset "crs": shape (), type "<i4">, None)

2018-04-04 20:33:13,472 DEBG 'pdp_backend' stdout output:
ERROR:pdp.error:Exception raised during streamed response: ()
None
```
* files with the mysterious data anomaly cannot be opened by hd5 in in python interpreter, and give the following error messages:
```
>>> f = h5py.File('bc_ppt_monthly_CAI_timeseries_19500101_20071231.nc')
>>> f.attrs.get('history')
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "/tmp/env/lib/python2.7/_abcoll.py", line 382, in get
    return self[key]
  File "h5py/_objects.pyx", line 54, in h5py._objects.with_phil.wrapper
  File "h5py/_objects.pyx", line 55, in h5py._objects.with_phil.wrapper
  File "/tmp/env/local/lib/python2.7/site-packages/h5py/_hl/attrs.py", line 81, in __getitem__
    attr.read(arr, mtype=htype)
  File "h5py/_objects.pyx", line 54, in h5py._objects.with_phil.wrapper
  File "h5py/_objects.pyx", line 55, in h5py._objects.with_phil.wrapper
  File "h5py/h5a.pyx", line 355, in h5py.h5a.AttrID.read
  File "h5py/_proxy.pyx", line 36, in h5py._proxy.attr_rw
IOError: Unable to read attribute (no appropriate function for conversion path)
```
* ncWMS returns this error when asked to serve the file:
```
Mar 27, 2018 8:26:15 AM ucar.nc2.iosp.hdf5.H5header$MessageAttribute read

SEVERE: bad version 72 at filePos 75864
```

When the file is examined with xxd, there is a lot of 0 padding around the position reported by ncWMS, which is inside the file attribute declaration section, but non-anomalous netCDF files contain padding too, so it's not clear whether that's the ultimate cause of the issue.

Several of the error messages given by various netCDF software reference attributes or warn about being unable to open attributes. However, removing one, or even all, of the attributes from the file does not fix the issue. My best guess is that some sort of index to the attributes inside the file is corrupted or incorrect, but I haven't been able to verify this.

The error can be fixed by converting the file from netCDF4 format to netCDF4/classic format, and then back to netCDF4 format. Perhaps, if the issue is really an internal index, this rebuilds the index from scratch in a way that deleting attributes does not.

### 3. Provide a usable coordinate reference system
The files contain a crs(time) variable with attributes that describe a coordinate system, which is a valid way to provide a coordinate reference system under the CF standards (though the standards call for dimensionless variables). Unfortunately, while the CRS data is valid, there are a couple problems with using this metadata as-is:
* the CRS is identified as a latitude-longitude system, but does not specify a value for semi minor axis, as required by nc_helpers
* when nc_helpers was modified to determine from projection from the grid mapping string attribute instead of axis attributes, it turns out that the projection these files were created with, NAD83, is not supported by older versions of ncWMS and cannot be displayed
* additionally, the pdp backend crashes when attempting to serve data from a dimensionless or dimensioned but null variable (even though such variables are supported by the CF standards for providing projection information)

So instead, the CRS variable was entirely removed, and the database was edited to assign these files to the same grid mappings as the other PRISM files already present in the database.

## Procedure

1. Add missing metadata with the update_metadata script from the modelmeta package
```
python scripts/update_metadata -u prism_metadata.yaml bc_ppt_monthly_CAI_timeseries_19500101_20071231.nc
python creation_date_from_history.py bc_ppt_monthly_CAI_timeseries_19500101_20071231.nc
```

2. Remove possibly error-causing history attribute, which is longer than legally allowed in netCDF
```
python scripts\update_metadata -y strip_history.yaml bc_ppt_monthly_CAI_timeseries_19500101_20071231.nc
```

3. Convert file (excluding the CRS variable) from netCDF4 to netCDF4/Classic format, then back to netCDF4 format, which corrects the unknown file format error
```
nccopy -7 -V lat,lon,time,pr bc_ppt_monthly_CAI_timeseries_19500101_20071231.nc temp.nc
nccopy -4 temp.nc bc_ppt_monthly_CAI_timeseries_19500101_20071231.nc
rm temp.bc
```

4. Index the file and assign to an ensemble as normal

5. Edit database to assign the files to the same grid mapping as the other PRISM files on the same portal (grid ID #4) so ncWMS can display them:
```
UPDATE data_file_variables SET grid_id = 4 WHERE grid_id = 20;
```

Repeat for tasmax and tasmin, changing the title in prism_metadata and variables in the nccopy call.

## Datafiles Processed With This Method
bc_ppt_monthly_CAI_timeseries_19500101_20071231.nc

bc_tmax_monthly_CAI_timeseries_19500101_20071231.nc

bc_tmin_monthly_CAI_timeseries_19500101_20071231.nc